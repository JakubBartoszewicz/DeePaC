[Devices]
# No. of available GPUs
N_GPUs = 0
# No. of available CPUs (for training)
N_CPUs = 8
# Allow GPU memory growth, instead of reserving it all
AllowGrowth = True
# Enable device parallelism for RC-networks
DeviceParallel = True
# Forward and build device
Device_fwd = /device:GPU:0
# Rev-comp device
Device_rc = /device:GPU:1
# Model build device
Device_build = /device:GPU:2


[DataLoad]
# Use generators to use data batch-by-batch, if it doesn't fit in memory
LoadTrainingByBatch = True
# Use generators to use data batch-by-batch, if it doesn't fit in memory
LoadValidationByBatch = True
# No. of threads to use for batch-by-batch loading
BatchWorkers = 4
# Batch queue size
BatchQueue = 128

[InputData]
# Read length
SeqLength = 250

[Architecture]
# Reverse complement architecture. none (no RC), siam (separate flow for each strand) or full (full RC mode)
RC_Mode = full
# No. of convolutional layers
N_Conv = 1
# No. of recurrent layers
N_Recurrent = 0
# No. of dense layers
N_Dense = 1
# Random seed for weight initialization
Seed = 0
# Use advanced Keras activations, e.g. PReLU (not implemented)
AdvancedActivations = False
# Weight initializer: he_uniform or glorot_uniform
WeightInit = he_uniform
# Input dropout rate
Input_Dropout = 0.0
# No. of convolutional filters
Conv_Units = 2
# Convolutional filter size
Conv_FilterSize = 3
# Activation function
Conv_Activation = relu
# Use batch normalization
Conv_BN = False
# Pooling mode: max, average, last_max, last_average or none
Conv_Pooling = max
# Dropout rate
Conv_Dropout = 0.0 
# No. of recurrent units
Recurrent_Units = 0
# Use batch normalization
Recurrent_BN = False
# Dropout rate
Recurrent_Dropout = 0.5
# Dense merge function:
#    maximum: motif on fwd fuzzy OR rc (Goedel t-conorm)
#    multiply: motif on fwd fuzzy AND rc (product t-norm)
#    add: motif on fwd PLUS/"OR" rc (Shrikumar-style)
#    average: motif on fwd PLUS/"OR" rc (Shrikumar-style), rescaled
Dense_Merge = add
# No. of units in the dense layer
Dense_Units = 2
# Activation function
Dense_Activation = relu
# Use batch normalization
Dense_BN = False
# Dropout rate
Dense_Dropout = 0.0

[ClassWeights]
# Use a weighting scheme
UseWeights = False
# Negative class count if needed
ClassCount_0 = 0
# Positive class count if needed
ClassCount_1 = 0

[Paths]
# Path to training data
TrainingData = deepac-tests/sample_train_data.npy
# Path to training labels
TrainingLabels = deepac-tests/sample_train_labels.npy
#Path to validation data
ValidationData = deepac-tests/sample_val_data.npy
# Path to validation labels
ValidationLabels = deepac-tests/sample_val_labels.npy
# A prefix used for the model and output files
RunName = deepac-test

[Training]
# First epoch (1-indexed)
EpochStart = 1
# Max last epoch, exclusive (1-indexed)
EpochEnd = 3
# Batch size
BatchSize = 512
# Early stopping patience
Patience = 10
# L2 regularization factor
Lambda_L2 = 0.0
# Learning rate
LearningRate = 0.001
# Keras optimizer
Optimizer = adam
# Log memory usage (rss)
MemUsageLog = True
# Print and plot summaries. Uses Keras for plotting, so requires graphviz and pydot
Summaries = False
# Logging path
LogPath = deepac-tests
# Use TensorBoard
Use_TB = False
# TensorBoard histogram freq
TBHistFreq = 1
