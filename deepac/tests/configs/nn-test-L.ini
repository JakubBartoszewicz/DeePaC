[Devices]
# Tensorflow distribution strategy
DistStrategy = MirroredStrategy
# Use old-style explicit build device placement instead of the distribution strategy
Simple_build = False
# Model build device (for OneDeviceStrategy and simple build)
Device_build = /device:GPU:0


[DataLoad]
# Use TFRecordDatasets
Use_TFData = False
# Alternatively: Use generators to use data batch-by-batch, if it doesn't fit in memory
LoadTrainingByBatch = True
# Use multiprocessing - disable if training freezes, keep as it is if it's fine
Multiprocessing = False
# No. of threads to use for batch-by-batch loading
BatchWorkers = 4
# Batch queue size
BatchQueue = 128

[InputData]
# Read length
SeqLength = 250
# Mask zeros (padding and Ns) in input.
# May cause problems with SHAP's DeepExplainer. If so, use --gradient in nt contibution maps (gwpa nt_contribs)
MaskZeros = False
# Use dynamic subreads. Supported only by Keras Sequences (Use_TFData = False, LoadTrainingByBatch = True)
UseSubreads = False
# Min subread length (closed interval [min,max])
MinSubreadLength = 50
# Max subread length (closed interval [min,max])
MaxSubreadLength = 250
# Subread length distribution (not implemented)
DistSubread = uniform

[Architecture]
# Number of classes
N_Classes = 4
# Reverse complement architecture. none (no RC), siam (separate flow for each strand) or full (full RC mode)
RC_Mode = full
# No. of convolutional layers (excluding bottlenecks)
N_Conv = 5
# Residual block (skip connection) size
Skip_Size = 2
# Bottlenecks
Bottlenecks = False
# Cardinality (as in ResNeXt)
Cardinality = 1
# No. of recurrent layers
N_Recurrent = 0
# No. of dense layers
N_Dense = 0
# Random seed for weight initialization
Seed = 0
# Global weight initializer: he_uniform or glorot_uniform. Overrides all below, unless "custom".
WeightInit = he_uniform
# Weight initializer: he_uniform or glorot_uniform. Convolutional layers
WeightInit_Conv = glorot_uniform
# Weight initializer: he_uniform or glorot_uniform. RC-merge dense layers
WeightInit_Merge = glorot_uniform
# Weight initializer: he_uniform or glorot_uniform. LSTM layers
WeightInit_LSTM = glorot_uniform
# Weight initializer: he_uniform or glorot_uniform. Dense layers
WeightInit_Dense = glorot_uniform
# Weight initializer: he_uniform or glorot_uniform. Output layer
WeightInit_Out = glorot_uniform
# Gain for the orthogonal recurrent initializer (change only if you know why)
OrthoGain = 1.0
# Input dropout rate
Input_Dropout = 0.25
# No. of convolutional filters
Conv_Units = 64,64,64,64,64
# No. of units in the last layer of a bottleneck block
Conv_Bottleneck_Units = 0
# Convolutional filter size
Conv_FilterSize = 7,5,5,5,5
# Dilation rate
Conv_Dilation = 1,1,1,1,1
# Strides
Conv_Stride = 2,1,2,1,2
# Activation function
Conv_Activation = relu
# Padding. "same" or "valid".
Conv_Padding = same
# Use batch normalization
Conv_BN = True
# Pooling mode: max, average, last_max, last_average, first_max_last_average or none.
# Max & last_max are incompatible with shap
Conv_Pooling = last_average
# Dropout rate
Conv_Dropout = 0.0
# No. of recurrent units
Recurrent_Units = 0
# Use batch normalization
Recurrent_BN = False
# Dropout rate
Recurrent_Dropout = 0.5
# Dense merge function:
#    maximum: motif on fwd fuzzy OR rc (Goedel t-conorm)
#    multiply: motif on fwd fuzzy AND rc (product t-norm)
#    add: motif on fwd PLUS/"OR" rc (Shrikumar-style)
#    average: motif on fwd PLUS/"OR" rc (Shrikumar-style), rescaled
Dense_Merge = add
# No. of units in the dense layer
Dense_Units = 256,256
# Activation function
Dense_Activation = relu
# Use batch normalization
Dense_BN = True
# Dropout rate
Dense_Dropout = 0.0
# Monte Carlo dropout at inference time
MC_Dropout = False

[TransferEmbeddings]
# Path to a compiled submodel to use for sequence embeddings. "none" to train the model from scratch
EmbeddingModel = none
# Freeze embedding weights (True) or don't freeze for finetuning (False)
FreezeEmbeddings = True
# Remove top N layers. Note that activations, lambdas, dropout etc. are counted as layers.
# Consult the model summary for details.
RemoveTopN = 6

[ArchitectureExtras]
# Reverse complement attention mode. False for "siam" attention, True for full-RC. No effect on non-RC models
Full_RC_Attention = True
# Reverse complement mode for the feed-forward transformer subnetwork. As above.
Full_RC_FFN = True
# Input dimension scale. Repeat the input this many times along the channel dimension for a "free" wider embedding
Scale_Input_Dim = 1
# Transformer blocks
Tformer_Blocks = 0
# Transformer attention heads
Tformer_Heads = 8,8,8,8,8,8
# Transformer feed-formard subnetwork dimension (d_ff)
Tformer_Dim = 32,32,32,32,32,32
# Transformer output embedding dimension (d_model)
Tformer_EDim = 32,32,32,32,32,32
# Dimensions of random projections for fast generalized attention (Performer). 0 to disable
Tformer_Performer_Dim = 32,32,32,32,32,32
# Transformer internal dropout rate
Tformer_Dropout = 0.1
# Use 1D RC-Conv to correct for output dimension doubling after each layer in full-RC transformers
# (does not apply to the first layer if the first layer is a transformer)
Tformer_Keep_Edim = True

[ClassWeights]
# Use a weighting scheme
UseWeights = False
# Class counts: negative,positive (for imbalanced data)
ClassCounts = 2048,2048
# Initialize bias of the final layer with log(pos/neg)
LogInit = False

[Paths]
# Path to training data
TrainingData = deepac-tests/sample_train_multi_data.npy
# Path to training labels
TrainingLabels = deepac-tests/sample_train_multi_labels.npy
#Path to validation data
ValidationData = deepac-tests/sample_val_multi_data.npy
# Path to validation labels
ValidationLabels = deepac-tests/sample_val_multi_labels.npy
# A prefix used for the model and output files
RunName = deepac-test

[Training]
# First epoch (1-indexed)
EpochStart = 1
# Max last epoch, inclusive (1-indexed)
EpochEnd = 1
# Batch size
BatchSize = 512
# Early stopping patience
Patience = 10
# L1 regularization factor
Lambda_L1 = 0.0
# L2 regularization factor
Lambda_L2 = 0.0
# Learning rate
LearningRate = 0.001
# Keras optimizer
Optimizer = adam
# Log memory usage (rss)
MemUsageLog = True
# Print and plot summaries. Uses Keras for plotting, so requires graphviz and pydot
Summaries = False
# Logging path
LogPath = deepac-tests
# Use TensorBoard
Use_TB = False
# TensorBoard histogram freq
TBHistFreq = 1
