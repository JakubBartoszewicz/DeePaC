[Devices]
# No. of available GPUs
N_GPUs = 1
# No. of available CPUs (for training)
N_CPUs = 120
# Allow GPU memory growth, instead of reserving it all
AllowGrowth = True

[DataLoad]
# Use generators to use data batch-by-batch, if it doesn't fit in memory
LoadByBatch = True
# No. of threads to use for batch-by-batch loading
BatchWorkers = 4
# Batch queue size
BatchQueue = 128

[InputData]
# Read length
SeqLength = 250

[Architecture]
# No. of convolutional layers
N_Conv = 0 
# No. of dense layers
N_Dense = 0 
# Random seed for weight initialization
Seed = 0
# Use advanced Keras activations, e.g. PReLU (not implemented)
AdvancedActivations = False
# Weight initializer: he_uniform or glorot_uniform
WeightInit = he_uniform
# No. of convolutional filters
Conv_Units = 512,512
# Convolutional filter size
Conv_FilterSize = 15,15
# Activation function
Conv_Activation = relu
# Use batch normalization
Conv_BN = False
# Pooling mode: max or average
Conv_Pooling = average
# No. of recurrent units
LSTM_Units = 256
# No. of units in the dense layer
Dense_Units = 0
# Activation function
Dense_Activation = relu
# Use batch normalization
Dense_BN = False
# Dropout rate
Dropout = 0.5

[ClassWeights]
# Use a weighting scheme
UseWeights = False
# Negative class weight if needed
ClassWeight_0 = None
# Positive class weight if needed
ClassWeight_1 = None

[Paths]
# Path to training data
TrainingData = SCRATCH_NOBAK/train_data_10e6.npy
# Path to training labels
TrainingLabels = SCRATCH_NOBAK/train_labels_10e6.npy
#Path to validation data
ValidationData = SCRATCH_NOBAK/val_data.npy
# Path to validation labels
ValidationLabels = SCRATCH_NOBAK/val_labels.npy
# A prefix used for the model and output files
RunName = lstm-test

[Training]
# No. of epochs
N_Epochs = 10
# Batch size
BatchSize = 512
# Keras optimizer
Optimizer = adam
# Log memory usage (maxrss)
MemUsageLog = True
